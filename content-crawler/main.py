# -*- coding: utf-8 -*-
# íŒŒì¼ ì¸ì½”ë”©: UTF-8
"""ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ í”Œë«í¼ì—ì„œ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³  ì•„ì¹´ì´ë¸Œì— ì €ì¥í•©ë‹ˆë‹¤.
ì§€ì›: ë„¤ì´ë²„ ë¸”ë¡œê·¸, Tistory, GitHub Pages, YouTube
"""

import json
import os
import sys
import argparse
from datetime import datetime, timezone
from typing import List, Dict

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from utils.secrets import load_environment
load_environment()

# ë¡œì»¬ ëª¨ë“ˆ import
from crawlers.naver_blog import NaverBlogCrawler
from crawlers.tistory_blog import TistoryBlogCrawler
from crawlers.github_pages import GitHubPagesCrawler
from crawlers.youtube import YouTubeCrawler
from archive_manager import ArchiveManager
from event_date_extractor import EventDateExtractor
from utils.telegram_notifier import TelegramNotifier
from utils.error_collector import ErrorCollector
from scheduler import DailyDigestScheduler

# ë²„ì „ ì •ë³´ (ì•„ì¹´ì´ë¸Œ ì—…ë°ì´íŠ¸ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•´)
CRAWLER_VERSION = "2.2"


def load_config(config_file="config.json"):
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(config_file):
        print(f"[!] ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
        return None

    with open(config_file, "r", encoding="utf-8") as f:
        return json.load(f)


def save_metadata(posts, output_file):
    """í¬ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    data = {
        "last_updated": datetime.now(timezone.utc).isoformat(),
        "total_posts": len(posts),
        "posts": posts,
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"[+] ë©”íƒ€ë°ì´í„° ì €ì¥: {output_file} ({len(posts)}ê°œ)")


def archive_posts(posts, archive_root="./archive", blog_id="default", summarize: bool = False, raw_dir: str = None):
    """í¬ìŠ¤íŠ¸ë¥¼ ì•„ì¹´ì´ë¸Œì— ì €ì¥í•©ë‹ˆë‹¤.

    raw_dirê°€ ì§€ì •ë˜ë©´ ê° í¬ìŠ¤íŠ¸ì˜ ì›ë³¸ HTML/Raw ë°ì´í„°ë¥¼ í•´ë‹¹ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤ (ë¡œì»¬ ì „ìš©).
    """
    archive_mgr = ArchiveManager(archive_root)
    extractor = EventDateExtractor()
    archived_count = 0

    for post in posts:
        try:
            title = post.get("title", "ì œëª© ì—†ìŒ")
            link = post.get("link", "")
            content = post.get("content", "")
            published = post.get("published", "")
            tags = post.get("tags") or []
            comments = post.get("summary") or ""

            # ì¤‘ë³µ í™•ì¸
            if archive_mgr.is_archived(link):
                print(f"[i] ì´ë¯¸ ì•„ì¹´ì´ë¸Œë¨, ê±´ë„ˆëœë‹ˆë‹¤: {link}")
                continue

            # ì´ë²¤íŠ¸ ë‚ ì§œ ì¶”ì¶œ
            extracted_dates = extractor.extract(content) if content else []

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            from utils.keyword_extractor import extract_keywords
            keywords = extract_keywords(content or "", top_n=5)

            # ì´ë¯¸ì§€ ë©”íƒ€ ì¶”ì¶œ í•¨ìˆ˜
            def extract_images(html_str):
                images = []
                try:
                    from bs4 import BeautifulSoup
                    import hashlib
                    import requests

                    soup = BeautifulSoup(html_str or "", "html.parser")
                    for img in soup.find_all("img", src=True):
                        url = img["src"]
                        desc = img.get("alt", "")
                        size = None
                        sha = None
                        try:
                            r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                            data = r.content
                            size = len(data)
                            sha = hashlib.sha256(data).hexdigest()
                        except Exception:
                            pass
                        images.append({
                            "url": url,
                            "description": desc,
                            "blog": blog_id,
                            "size": size,
                            "sha256": sha,
                        })
                except Exception:
                    pass
                return images

            # ìš”ì•½ ìš”ì²­ ì‹œ
            summary_text = ""
            if summarize and content:
                try:
                    summary_text = archive_mgr.summarize_with_ollama(content) or ""
                    if summary_text:
                        print(f"[+] Ollama ìš”ì•½ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    print(f"[!] ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

            # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            html_for_images = post.get("html") or content or ""
            images = extract_images(html_for_images)

            # raw_html ì¤€ë¹„ (ìˆìœ¼ë©´ html ì €ì¥)
            raw_html = post.get("html") or ""

            # ë§ˆí¬ë‹¤ìš´ ìƒì„± ë° ì €ì¥ (create_markdown_file ì‚¬ìš©)
            archive_mgr.create_markdown_file(
                title=title,
                url=link,
                content=content or "(ë³¸ë¬¸ ì—†ìŒ)",
                created_at=published,
                event_dates=extracted_dates,
                tags=tags,
                comments=summary_text or comments,
                keywords=keywords,
                crawler_version=CRAWLER_VERSION,
                images=images,
                raw_html=raw_html,
                raw_dir=raw_dir,
            )
            archived_count += 1

        except Exception as e:
            print(f"[!] ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨ ({title}): {e}")

    print(f"[+] {archived_count}ê°œ í¬ìŠ¤íŠ¸ë¥¼ ì•„ì¹´ì´ë¸Œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    archive_mgr.update_index(blog_id, platform="mixed")


def crawl_naver_blog(config: Dict, args):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§

    config í”Œë«í¼ ì„¹ì…˜ì—ì„œ blogs ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ì–´ ì—¬ëŸ¬ ë¸”ë¡œê·¸ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
    ë˜í•œ CLIì˜ --naver-blog ì˜µì…˜ì„ í†µí•´ ìˆ˜ë™ ë¸”ë¡œê·¸ IDë¥¼ ì¶”ê°€ë¡œ í¬ë¡¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    naver_config = config.get("platforms", {}).get("naver_blog", {})
    if not naver_config.get("enabled"):
        return []

    # ê¸°ë³¸ ë¸”ë¡œê·¸ ëª©ë¡
    blogs = []
    if isinstance(naver_config.get("blogs"), list):
        blogs = naver_config.get("blogs")
    else:
        # ì´ì „ êµ¬ì¡° í˜¸í™˜ì„±
        blogs = [{
            "blog_id": naver_config.get("blog_id", "boyinblue"),
            "rss_url": naver_config.get("rss_url"),
            "request_interval_seconds": naver_config.get("request_interval_seconds", 1.0),
        }]

    # CLIì— ì¶”ê°€ëœ ë¸”ë¡œê·¸ ID ì²˜ë¦¬
    if args.naver_blog:
        for bid in args.naver_blog:
            blogs.append({"blog_id": bid, "rss_url": None, "request_interval_seconds": 1.0})

    all_posts = []
    archive_root = config.get("archive_root", "./archive")
    raw_dir = config.get("raw_directory")

    for info in blogs:
        blog_id = info.get("blog_id")
        request_interval = info.get("request_interval_seconds", 1.0)
        rss_url = info.get("rss_url")

        print(f"\n[*] ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘ (ë¸”ë¡œê·¸: {blog_id})")
        print(f"[*] ìš”ì²­ ê°„ê²©: {request_interval}ì´ˆ")

        crawler = NaverBlogCrawler(blog_id, rss_url=rss_url, request_interval=request_interval)
        posts = crawler.crawl(
            fetch_content=args.fetch_content,
            max_posts=args.max_posts,
            full=args.full,
            follow_internal=args.follow_internal,
        )

        if posts:
            archive_posts(posts, archive_root, blog_id, summarize=args.summarize, raw_dir=raw_dir)
            all_posts.extend(posts)

    return all_posts


def crawl_tistory_blogs(config: Dict, args):
    """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
    tistory_config = config.get("platforms", {}).get("tistory", {})
    if not tistory_config.get("enabled"):
        return []

    blogs = tistory_config.get("blogs", [])
    all_posts: List[Dict] = []

    for blog_info in blogs:
        blog_url = blog_info.get("blog_url")
        blog_name = blog_info.get("name", "unknown")
        request_interval = blog_info.get("request_interval_seconds", 1.0)

        if not blog_url:
            continue

        print(f"\n[*] í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘ ({blog_name}: {blog_url})")
        print(f"[*] ìš”ì²­ ê°„ê²©: {request_interval}ì´ˆ")

        crawler = TistoryBlogCrawler(blog_url, request_interval=request_interval)
        posts = crawler.crawl(max_posts=args.max_posts, use_sitemap=args.use_sitemap)

        if posts:
            archive_root = config.get("archive_root", "./archive")
            raw_dir = config.get("raw_directory")
            archive_posts(posts, archive_root, blog_name, summarize=args.summarize, raw_dir=raw_dir)
            all_posts.extend(posts)

    return all_posts


def crawl_github_pages(config: Dict, args):
    """GitHub Pages í¬ë¡¤ë§"""
    gp_config = config.get("platforms", {}).get("github_pages", {})
    if not gp_config.get("enabled"):
        return []

    blogs = gp_config.get("blogs", [])
    all_posts: List[Dict] = []

    for blog_info in blogs:
        blog_url = blog_info.get("blog_url")
        blog_name = blog_info.get("name", "unknown")
        request_interval = blog_info.get("request_interval_seconds", 1.0)

        if not blog_url:
            continue

        print(f"\n[*] GitHub Pages í¬ë¡¤ë§ ì‹œì‘ ({blog_name}: {blog_url})")
        print(f"[*] ìš”ì²­ ê°„ê²©: {request_interval}ì´ˆ")

        crawler = GitHubPagesCrawler(blog_url, request_interval=request_interval)
        posts = crawler.crawl(fetch_content=args.fetch_content, max_posts=args.max_posts)

        if posts:
            archive_root = config.get("archive_root", "./archive")
            raw_dir = config.get("raw_directory")
            archive_posts(posts, archive_root, blog_name, summarize=args.summarize, raw_dir=raw_dir)
            all_posts.extend(posts)

    return all_posts


def crawl_youtube(config: Dict, args):
    """YouTube ì±„ë„ í¬ë¡¤ë§"""
    yt_config = config.get("platforms", {}).get("youtube", {})
    if not yt_config.get("enabled"):
        return []

    channels = yt_config.get("channels", [])
    all_videos: List[Dict] = []

    for channel_info in channels:
        channel_url = channel_info.get("channel_url")
        channel_name = channel_info.get("name", "unknown")
        request_interval = channel_info.get("request_interval_seconds", 1.0)

        if not channel_url:
            continue

        print(f"\n[*] YouTube ì±„ë„ í¬ë¡¤ë§ ì‹œì‘ ({channel_name}: {channel_url})")
        print(f"[*] ìš”ì²­ ê°„ê²©: {request_interval}ì´ˆ")

        crawler = YouTubeCrawler(channel_url=channel_url, request_interval=request_interval)
        videos = crawler.crawl(max_videos=args.max_posts)

        if videos:
            archive_root = config.get("archive_root", "./archive")
            raw_dir = config.get("raw_directory")
            archive_posts(videos, archive_root, channel_name, summarize=args.summarize, raw_dir=raw_dir)
            all_videos.extend(videos)

    return all_videos


def test_telegram_config():
    """í…”ë ˆê·¸ë¨ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    from utils.secrets import get_telegram_token, get_telegram_chat_id
    
    print("\n[*] í…”ë ˆê·¸ë¨ ì„¤ì • í™•ì¸ ì¤‘...")
    token = get_telegram_token()
    chat_id = get_telegram_chat_id()
    
    print(f"  TELEGRAM_BOT_TOKEN: {'âœ“ ì„¤ì •ë¨' if token and token != 'your_telegram_bot_token_here' else 'âœ— ì„¤ì • í•„ìš”'}")
    print(f"  TELEGRAM_CHAT_ID: {'âœ“ ì„¤ì •ë¨' if chat_id else 'âœ— ì„¤ì • í•„ìš”'}")
    
    if not token or token == 'your_telegram_bot_token_here':
        print("\n[i] í…”ë ˆê·¸ë¨ ë´‡ í† í°ì„ ì„¤ì •í•˜ë ¤ë©´:")
        print("  1. Telegramì—ì„œ @BotFatherì™€ ëŒ€í™”")
        print("  2. /newbot ëª…ë ¹ì–´ë¡œ ìƒˆ ë´‡ ìƒì„±")
        print("  3. ì–»ì€ í† í°ì„ .env íŒŒì¼ì˜ TELEGRAM_BOT_TOKENì— ì…ë ¥")
        return False
    
    if not chat_id:
        print("\n[i] í…”ë ˆê·¸ë¨ ì±„íŒ… IDë¥¼ ì„¤ì •í•˜ë ¤ë©´ .env íŒŒì¼ì˜ TELEGRAM_CHAT_IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return False
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸
    print("\n[*] í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë°œì†¡ í…ŒìŠ¤íŠ¸...")
    notifier = TelegramNotifier()
    
    if not notifier.is_configured():
        print("[!] í…”ë ˆê·¸ë¨ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    success = notifier.send_message("ğŸ”” <b>í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€</b>\n\nì½˜í…ì¸  í¬ë¡¤ëŸ¬ í…”ë ˆê·¸ë¨ ì„¤ì •ì´ ì •ìƒì‘ë™í•©ë‹ˆë‹¤!")
    if success:
        print("[+] í…”ë ˆê·¸ë¨ ì„¤ì • ì„±ê³µ!")
    else:
        print("[!] í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨ - í† í°ì´ë‚˜ ì±„íŒ… IDë¥¼ í™•ì¸í•˜ì„¸ìš”")

    return success


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì½˜í…ì¸  í¬ë¡¤ëŸ¬ v2.1 - ë‹¤ì¤‘ í”Œë«í¼ ì§€ì›")
    parser.add_argument(
        "--no-archive",
        action="store_true",
        help="ì•„ì¹´ì´ë¸Œì— ì €ì¥í•˜ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥",
    )
    parser.add_argument(
        "--fetch-content",
        action="store_true",
        help="í¬ìŠ¤íŠ¸ ë³¸ë¬¸ë„ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)",
    )
    parser.add_argument(
        "--max-posts",
        type=int,
        default=None,
        help="ìµœëŒ€ í¬ë¡¤ë§ í¬ìŠ¤íŠ¸ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)",
    )
    parser.add_argument(
        "--summarize",
        action="store_true",
        help="Ollamaë¥¼ ì‚¬ìš©í•´ ê° í¬ìŠ¤íŠ¸ ìš”ì•½ ì¶”ê°€",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="RSSì— ì—†ëŠ” ì´ì „ ê¸€ê¹Œì§€ ìŠ¤í¬ë ˆì´í•‘í•´ ìµœëŒ€í•œ ë§ì´ ìˆ˜ì§‘",
    )
    parser.add_argument(
        "--follow-internal",
        action="store_true",
        help="í¬ë¡¤ëœ í¬ìŠ¤íŠ¸ ë‚´ì— ìˆëŠ” ê°™ì€ ë¸”ë¡œê·¸ì˜ ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ ë§í¬ë„ í•¨ê»˜ ë”°ë¼ê°‘ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--use-sitemap",
        action="store_true",
        help="í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì˜ sitemap.xmlì— ìˆëŠ” ë§í¬ë¥¼ ì¶”ê°€ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--schedule",
        action="store_true",
        help="ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰ (ì¼ì¼ ë‹¤ì´ì œìŠ¤íŠ¸)",
    )
    parser.add_argument(
        "--test-telegram",
        action="store_true",
        help="í…”ë ˆê·¸ë¨ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
    )
    parser.add_argument(
        "--no-error-report",
        action="store_true",
        help="ì—ëŸ¬ê°€ ë°œìƒí•´ë„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë³´ê³ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
    )
    parser.add_argument(
        "--naver-blog",
        action="append",
        help="ì¶”ê°€ë¡œ ìˆ˜ë™ í¬ë¡¤ë§í•  ë„¤ì´ë²„ ë¸”ë¡œê·¸ IDë¥¼ ì§€ì • (ì—¬ëŸ¬ ê°œ ë°˜ë³µ ê°€ëŠ¥)",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ì½˜í…ì¸  í¬ë¡¤ëŸ¬ v2.1")
    print("=" * 60)
    print()

    # í…”ë ˆê·¸ë¨ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test_telegram:
        test_telegram_config()
        return

    # ì„¤ì • ë¡œë“œ
    config = load_config("config.json")
    if not config:
        return

    # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
    if args.schedule:
        notifier = TelegramNotifier()
        scheduler_config = config.get("scheduler", {})
        scheduler = DailyDigestScheduler(scheduler_config, notifier)
        scheduler.start()
        return

    # í¬ë¡¤ë§ ëª¨ë“œ
    print("[*] í¬ë¡¤ë§ ì‹œì‘...")
    all_posts: List[Dict] = []
    
    # ì—ëŸ¬ ìˆ˜ì§‘ ì‹œì‘
    error_collector = ErrorCollector()
    
    with error_collector:
        # ê° í”Œë«í¼ë³„ í¬ë¡¤ë§
        if not args.no_archive:
            naver_posts = crawl_naver_blog(config, args)
            all_posts.extend(naver_posts)

            tistory_posts = crawl_tistory_blogs(config, args)
            all_posts.extend(tistory_posts)

            github_posts = crawl_github_pages(config, args)
            all_posts.extend(github_posts)

            youtube_videos = crawl_youtube(config, args)
            all_posts.extend(youtube_videos)

    print("\n" + "=" * 60)
    print(f"[+] ëª¨ë“  ì‘ì—… ì™„ë£Œ! (ì´ {len(all_posts)}ê°œ í•­ëª© ìˆ˜ì§‘)")
    if error_collector.has_errors():
        print(f"[!] ì—ëŸ¬ {len(error_collector.errors)}ê°œ ë°œìƒ")
    print("=" * 60)
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ì˜µì…˜ìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥)
    if error_collector.has_errors() and not args.no_error_report:
        notifier = TelegramNotifier()
        if notifier.is_configured():
            notifier.send_errors(error_collector.errors)
        else:
            print("[!] í…”ë ˆê·¸ë¨ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì—ëŸ¬ë¥¼ ì „ì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif error_collector.has_errors() and args.no_error_report:
        print("[i] ì—ëŸ¬ ë¦¬í¬íŒ…ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
